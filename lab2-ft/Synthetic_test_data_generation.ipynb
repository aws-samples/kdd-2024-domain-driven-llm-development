{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca6cfc0",
   "metadata": {},
   "source": [
    "# Hands-on Tutorial\n",
    "## Domain-Driven LLM Development: Insights into RAG and Fine-Tuning Practices\n",
    "### Lab 2.1 : Synthetic Test Data Generation \n",
    "#### Summary: \n",
    "This lab focused on generating synthetic data for testing the fine-tuned model in the next lab. We use Claude3 Sonnet on Amazon Bedrock. \n",
    "\n",
    "- The question-context-answer pairs provided by CUAD dataset are used as \"seed data\"     \n",
    "- The generated dataset are in the format [context, seed_question, question, answer]    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f250ec8",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ee1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a51028",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_bedrock = boto3.client(service_name=\"bedrock\", region_name=\"us-west-2\")\n",
    "boto3_bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-west-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2cd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA_Gen_Bedrock(model_id,model_kwargs,prompt):\n",
    "                \n",
    "    input_token = len(prompt.split())/0.75\n",
    "\n",
    "    if ('titan' in model_id):    \n",
    "        model_body = {\n",
    "            \"inputText\": f\"{prompt}\"\n",
    "        }\n",
    "        model_body[\"textGenerationConfig\"] =  model_kwargs  \n",
    "    elif ('claude-3' in model_id):\n",
    "        model_body = {\n",
    "                        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                        \"max_tokens\": 1024,\n",
    "                        \"messages\": [\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "                            }\n",
    "                        ],\n",
    "        }\n",
    "    else:\n",
    "        model_body = {\n",
    "            \"prompt\": f\"{prompt}\"\n",
    "        }\n",
    "        model_body.update(model_kwargs)\n",
    "\n",
    "    body_bytes = json.dumps(model_body).encode('utf-8')\n",
    "\n",
    "    st = time.time()\n",
    "\n",
    "    if ('claude-3' in model_id):\n",
    "        response = boto3_bedrock_runtime.invoke_model(\n",
    "                    modelId=model_id,\n",
    "                    body=body_bytes,\n",
    "                )\n",
    "    else:\n",
    "        response = boto3_bedrock_runtime.invoke_model(\n",
    "                    modelId=model_id,\n",
    "                    contentType=\"application/json\",\n",
    "                    accept=\"*/*\",\n",
    "                    body=body_bytes,\n",
    "                )\n",
    "\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "\n",
    "    if ('titan' in model_id):\n",
    "        response_body_json = json.loads(response['body'].read().decode('utf-8'))\n",
    "        llm_response = response_body_json[\"results\"][0][\"outputText\"].strip()\n",
    "        llm_latency = response[\"ResponseMetadata\"][\"HTTPHeaders\"][\"x-amzn-bedrock-invocation-latency\"]\n",
    "    elif ('llama' in model_id):\n",
    "        response_body_json = json.loads(response['body'].read().decode('utf-8'))\n",
    "        llm_response = response_body_json[\"generation\"].strip()\n",
    "    elif ('claude-v2' in model_id or 'claude-instant-v1' in model_id ):\n",
    "        response_body_json = json.loads(response['body'].read().decode('utf-8'))\n",
    "        llm_response = response_body_json[\"completion\"].strip()\n",
    "    elif ('claude-3' in model_id):\n",
    "        response_body_json = json.loads(response['body'].read().decode('utf-8'))\n",
    "        llm_response = response_body_json[\"content\"][0][\"text\"].strip()\n",
    "    elif ('mistral' in model_id):\n",
    "        response_body_json = json.loads(response['body'].read().decode('utf-8'))\n",
    "        llm_response = response_body_json[\"outputs\"][0][\"text\"].strip()    \n",
    "    else :\n",
    "        llm_response = 'MODEL TYPE NOT YET SUPPORTED.'\n",
    "    \n",
    "    output_token = len(llm_response.split())/0.75\n",
    "\n",
    "    throuput = output_token/elapsed_time\n",
    "    \n",
    "    return llm_response, elapsed_time, input_token, output_token, throuput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71276442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_strings_recursive(test_str, tag):\n",
    "    try:\n",
    "        # finding the index of the first occurrence of the opening tag\n",
    "        start_idx = test_str.find(\"<\" + tag + \">\")\n",
    "\n",
    "        # base case\n",
    "        if start_idx == -1:\n",
    "            return []\n",
    "\n",
    "        # extracting the string between the opening and closing tags\n",
    "        end_idx = test_str.find(\"</\" + tag + \">\", start_idx)\n",
    "        res = [test_str[start_idx+len(tag)+2:end_idx]]\n",
    "\n",
    "        # recursive call to extract strings after the current tag\n",
    "        res += extract_strings_recursive(test_str[end_idx+len(tag)+3:], tag)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    except:\n",
    "        return \"bad format\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40008348",
   "metadata": {},
   "source": [
    "### Prepare for seed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c508f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from json\n",
    "JSON_FILE = '../lab-data/CUAD_v1.json'\n",
    "\n",
    "with open(JSON_FILE, 'r') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128571e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_file(file_name, file_index):\n",
    "    question_list = []\n",
    "    input_list = []\n",
    "    answer_list = []\n",
    "    qa_id_list = []\n",
    "    context_list = []\n",
    "\n",
    "    for i in range(len((df['data'][file_index])['paragraphs'][0]['qas'])):\n",
    "        qas_input = df['data'][file_index]['paragraphs'][0]['qas'][i]['question']\n",
    "        qas_question = qas_input.split(\"Details: \")[1].strip()\n",
    "        if '?' not in qas_question:\n",
    "            qas_question = \"What is \"+qas_question+'?'        \n",
    "        qa_id = df['data'][file_index]['paragraphs'][0]['qas'][i]['id']\n",
    "        answer = ''\n",
    "        for j in range(len(df['data'][file_index]['paragraphs'][0]['qas'][i]['answers'])):\n",
    "            answer = answer + df['data'][file_index]['paragraphs'][0]['qas'][i]['answers'][j]['text'] + ', '\n",
    "        answer = answer[:len(answer)-2]\n",
    "        \n",
    "        question_list.append(qas_question)\n",
    "        answer_list.append(answer)\n",
    "        input_list.append(qas_input)\n",
    "        qa_id_list.append(qa_id)\n",
    "            \n",
    "    # build dataframe\n",
    "    df_seed_data = pd.DataFrame()\n",
    "    df_seed_data['question'] = question_list\n",
    "    df_seed_data['input'] = input_list    \n",
    "    df_seed_data['answer'] = answer_list\n",
    "    df_seed_data['qa_id'] = qa_id_list\n",
    "\n",
    "    # remove nan answer\n",
    "    df_seed_data = df_seed_data[df_seed_data['answer']!='']\n",
    "    df_seed_data = df_seed_data.reset_index()\n",
    "    \n",
    "    QA_FILE = '../lab-data/' + file_name + '_qa.csv'\n",
    "\n",
    "    df_seed_data.to_csv(QA_FILE, encoding='utf-8', sep=',', index=False)\n",
    "    \n",
    "    return len(df_seed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31eadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_INDEX = 38\n",
    "FILE_NAME = 'ENERGOUSCORP'\n",
    "row_num = create_qa_file(FILE_NAME, FILE_INDEX)\n",
    "row_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c397d0",
   "metadata": {},
   "source": [
    "### Define prompt for synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584cae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_trngen = \"\"\"\n",
    "Human:\n",
    "\n",
    "You are an AI assistant, your task is to generate question-answer pair from the given context. \n",
    "\n",
    "Analyze the context within the <context> XML tag and the seed question in <seed> XML tag, \n",
    "generate one question that rephrases the seed question within the <seed> XML tag. \n",
    "Make sure the generated questions are also relevant to the context within the <context> XML tag. \n",
    "\n",
    "In your response, present the question within the <question> tag.\n",
    "DO NOT nest <question> element. \n",
    "DO NOT put any extra attribute in the <question> tag. \n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<seed>\n",
    "{seed_question}\n",
    "</seed>\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_trngen = PromptTemplate(template=prompt_template_trngen, input_variables=[\"context\",\"seed_question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3378ca",
   "metadata": {},
   "source": [
    "### Load seed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df571b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"../lab-data/ENERGOUSCORP_qa.csv\"\n",
    "df_input = pd.read_csv (INPUT_FILE)\n",
    "df_input.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef24d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = df_input.input.values.tolist()\n",
    "question_list  = df_input.question.values.tolist()\n",
    "answer_list  = df_input.answer.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(question_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad42c5",
   "metadata": {},
   "source": [
    "### Generate testing data from a random seed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd58909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'anthropic.claude-3-sonnet-20240229-v1:0' \n",
    "\n",
    "model_kwargs = {\n",
    "        \"max_tokens\": 1024,\n",
    "        \"top_p\": 0.95,\n",
    "        \"temperature\": 0.05\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b065c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PROMPT_trngen.format(context = context_list[25], seed_question = question_list[25])\n",
    "\n",
    "qa_response = QA_Gen_Bedrock(model_id,model_kwargs,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d2dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract_strings_recursive(qa_response[0], \"question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fdf94",
   "metadata": {},
   "source": [
    "### Generate training/testing data in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b606c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_context_list = []\n",
    "val_seed_list = []\n",
    "val_question_list = []\n",
    "val_answer_list = []\n",
    "\n",
    "for i in range(len(question_list)):\n",
    "   \n",
    "    print(i+1,end=': ')\n",
    "    prompt = PROMPT_trngen.format(context = context_list[i], seed_question = question_list[i])\n",
    "\n",
    "    qa_response = QA_Gen_Bedrock(model_id,model_kwargs,prompt)\n",
    "\n",
    "    res_q = extract_strings_recursive(qa_response[0], \"question\")\n",
    "    \n",
    "    if \"bad format\" in res_q or len(res_q)==0:\n",
    "        pass\n",
    "    else:\n",
    "        val_context_list.append(context_list[i])\n",
    "        val_seed_list.append(question_list[i])\n",
    "        val_question_list.append(res_q[0])        \n",
    "        val_answer_list.append(answer_list[i])\n",
    "        print('*',end='')\n",
    "        \n",
    "print(\"\\nCompleted: generated \", len(question_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4709afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_question_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c6073b",
   "metadata": {},
   "source": [
    "### Store generated dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2bc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_FILE = \"../lab-data/ENERGOUSCORP_qa_test.csv\"  \n",
    "\n",
    "df_val_dataset = pd.DataFrame()  \n",
    "\n",
    "df_val_dataset[\"context\"] = val_context_list\n",
    "df_val_dataset[\"seed_question\"] = val_seed_list\n",
    "df_val_dataset[\"question\"] = val_question_list\n",
    "df_val_dataset[\"answer\"] = val_answer_list\n",
    "\n",
    "df_val_dataset.to_csv(VAL_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b2eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e21a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
