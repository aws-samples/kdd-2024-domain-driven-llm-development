{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Tutorial\n",
    "## Domain-Driven LLM Development: Insights into RAG and Fine-Tuning Practices\n",
    "### Lab3: RAG + Fine-tuning and Benchmarking\n",
    "#### Summary: \n",
    "This lab explore tools to support deciding between RAG and domain fine-tuning for LLM's tasks such as Q&A or summarizing documents.\n",
    "\n",
    "Based on results from `Lab 1 - Advanced Techniques in Retrieval Augmented Generation (RAG)`, and `Lab 2 - LLM fine-tuning`, we wil explore how we can compare both approaches to support decison making related to which solution to choose.\n",
    "\n",
    "Decision making related to choosing between Fine-tuned and RAG models performances should take into account model performance as well as pricing. \n",
    "\n",
    "In the first sesssion of this notebook we compare model performance to assess if one or both achieve a minimum aceptable performance based on appropriate metrics realted to the task on hand.\n",
    "\n",
    "The second session is about offering pricing analysis tools to help decide between both models.\n",
    "## Important note\n",
    "### The pricing values used here are simulated and DO NOT reflect any real AWS pricing\n",
    "The values used are consistent with the references below: \n",
    "> https://aws.amazon.com/bedrock/pricing/ </br>\n",
    "> https://aws.amazon.com/kendra/pricing/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing and importing nedeed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -qU pinecone-client==2.2.1 ipywidgets==7.0.0\n",
    "# install packages needed for plotting\n",
    "! pip install -U kaleido --quiet\n",
    "! pip install plotly --quiet\n",
    "! pip install spacy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load RAG and fine-tuned data from lab 1 and lab 2\n",
    "These files contain samples with evaluation metrics as well as associated prompts used. \n",
    "\n",
    "For this short lab we are considering 30 interaction samples from each model, with the prompts and associated metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINETUNED_FILE = \"../lab-data/\"+\"ft_evaldata.csv\"  \n",
    "#RAG_FILE = \"../lab-data/\"+\"rag_evaldata.csv\"  \n",
    "FINETUNED_FILE = \"../lab-data/\"+\"sft_trn_result.csv\"  \n",
    "RAG_FILE = \"../lab-data/\"+\"naive_rag_result.csv\"\n",
    "\n",
    "\n",
    "df_rag = pd.read_csv(RAG_FILE)\n",
    "df_finetuned = pd.read_csv(FINETUNED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetuned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we get the average metrics over all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric averages\n",
    "def get_metric_averages(metric_df):\n",
    "    # return list of metrics\n",
    "    average_metrics = metric_df.loc[:, ['semantic_similarity', 'token_overlap_recall','rouge_l_recall']].mean()\n",
    "    return [average_metrics.iloc[0],\n",
    "            average_metrics.iloc[1],\n",
    "            average_metrics.iloc[2]\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_finetuned_metrics = get_metric_averages(df_finetuned)\n",
    "print (\"semantic_similarity:\", l_finetuned_metrics[0],\n",
    "       \"\\ntoken_overlap_recall:\", l_finetuned_metrics[1],\n",
    "       \"\\nrouge_l_recall:\", l_finetuned_metrics[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rag_metrics = get_metric_averages(df_rag)\n",
    "print (\"semantic_similarity:\", l_rag_metrics[0],\n",
    "       \"\\ntoken_overlap_recall:\", l_rag_metrics[1],\n",
    "       \"\\nrouge_l_recall:\", l_rag_metrics[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 1 - Performance analysis\n",
    "#### Plot RAG vs fine-tuned performance metrics\n",
    "We use a radar plot as a easy way to present all metrics together for comparison.\n",
    "We are using 3 metrics in this hands-on:\n",
    "+ semantic_similarity\n",
    "+ token_overlap_recall\n",
    "+ rouge_l_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of name, degree, score\n",
    "nme = [\"semantic_similarity\", \"token_overlap_recall\", \"rouge_l_recall\",\n",
    "      \"semantic_similarity\", \"token_overlap_recall\", \"rouge_l_recall\"\n",
    "      ]\n",
    "deg = [\"finetuned\", \"finetuned\", \"finetuned\", \"rag\", \"rag\", \"rag\"]\n",
    "scr = [l_rag_metrics[0], l_rag_metrics[1], l_rag_metrics[2],\n",
    "       l_finetuned_metrics[0], l_finetuned_metrics[1], l_finetuned_metrics[2] \n",
    "       ]\n",
    " \n",
    "# dictionary of lists \n",
    "dict = {'metric': nme, 'model': deg, 'value': scr} \n",
    "   \n",
    "df_evaluation = pd.DataFrame(dict)\n",
    "   \n",
    "df_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metrics(df):\n",
    "    #df = pd.read_csv(\"../lab-data/\"+\"new_test.csv\")\n",
    "    fig = px.line_polar(df, r=\"value\",\n",
    "                        theta=\"metric\",\n",
    "                        color=\"model\",\n",
    "                        line_close=True,\n",
    "                        color_discrete_sequence=[\"#00eb93\", \"#4ed2ff\"],\n",
    "                        template=\"plotly_dark\")\n",
    "\n",
    "    fig.update_polars(angularaxis_showgrid=False,\n",
    "                      radialaxis_gridwidth=0,\n",
    "                      gridshape='linear',\n",
    "                      bgcolor=\"#494b5a\",\n",
    "                      radialaxis_showticklabels=True\n",
    "                      )\n",
    "    fig.write_image(f\"../lab-data/radarplot.pdf\")\n",
    "    \n",
    "    fig.update_layout(paper_bgcolor=\"#2c2f36\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compare metrics for both models\n",
    "visualize_metrics(df_evaluation);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis considerations\n",
    "This plot allows you to have a quick view of the key metrics considered for this task. \n",
    "\n",
    "Before going through a cost analysis it is important to make sure if the models are above expected performance thresholds for the important metrics related to your use case. \n",
    "\n",
    "As long as both are above adequate thresholds, tyou can proceed to the costs analysis.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 2 - Cost analysis\n",
    "Once both models paseed the test related to performance, we can look at the costs involved.\n",
    "\n",
    "We assume some premisses to proceed with this example:\n",
    "+ As we need to use small datasets to run these labs, we will extrapolate RAG dataset sizes for a more realistic scenario\n",
    "+ We also define a number of access per month for both solutions in order to calculate expected costs\n",
    "+ Princing considered for RAG model:\n",
    "    + Monthly costs related to the number of average input and output tokens\n",
    "    + Monthly hosting costs for the RAG datasets\n",
    "    + Monthly input and output token usage\n",
    "    + We are considering pricing based on hte use of Amazon Kendra\n",
    "+ Pricinng considered for Fine-tuned model\n",
    "    + Fixed training cost\n",
    "    + Monthly model hosting\n",
    "    + Monthly input and output token usage\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the input and output prompts from RAG and fine-tuned datasets form lab 1 and lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetuned_prompts = df_finetuned[[\"question\", \"response\"]]\n",
    "df_finetuned_prompts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rag_prompts = df_rag[[\"question\", \"response\"]]\n",
    "df_rag_prompts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting tokens from RAG and FT input and output prompts\n",
    "We now proceed to counting the input and output tokens and calculating the sample mean for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to import spacy ADD ABOVE!!\n",
    "import spacy \n",
    "# Creating blank language object then \n",
    "# tokenizing words of the sentence \n",
    "nlp = spacy.blank(\"en\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_finetuned_prompts['input_prompt_tokens'] = df_finetuned_prompts['input_prompt'].apply()\n",
    "pd.options.mode.copy_on_write = True\n",
    "df_finetuned_prompts['num_input_prompt'] = df_finetuned_prompts['question'].map(lambda a: len(nlp(a)))\n",
    "df_finetuned_prompts['num_output_prompt'] = df_finetuned_prompts['response'].map(lambda a: len(nlp(a)))\n",
    "\n",
    "# below is using context only. Noit neede\n",
    "# df_rag_prompts['num_input_prompt'] = df_rag_prompts['ctx_input_prompt'].map(lambda a: len(nlp(a)))\n",
    "df_rag_prompts['num_input_prompt'] = df_rag_prompts['question'].map(lambda a: len(nlp(a)))\n",
    "df_rag_prompts['num_output_prompt'] = df_rag_prompts['response'].map(lambda a: len(nlp(a)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_finetuned_prompts[['num_input_prompt']].mean(axis=1)\n",
    "ft_mean_input_size = df_finetuned_prompts.loc[:, 'num_input_prompt'].mean()\n",
    "ft_mean_output_size = df_finetuned_prompts.loc[:, 'num_output_prompt'].mean()\n",
    "rag_mean_input_size = df_rag_prompts.loc[:, 'num_input_prompt'].mean()\n",
    "rag_mean_output_size = df_rag_prompts.loc[:, 'num_output_prompt'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of name, degree, score\n",
    "model = [\"finetuned\", \"finetuned\", \"rag\", \"rag\"]\n",
    "prompt = [\"input\", \"output\", \"input\", \"output\"]\n",
    "num_tokens = [ft_mean_input_size, ft_mean_output_size, rag_mean_input_size, rag_mean_output_size]\n",
    " \n",
    "# dictionary of lists \n",
    "dict = {'prompt_type': prompt, 'model': model, 'num_tokens': num_tokens} \n",
    "   \n",
    "df_comp_in_out = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing prompt sizes for RAG and Fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df_comp_in_out, title = \"Input and output mean token sizes\", x=\"prompt_type\", y=\"num_tokens\",\n",
    "             color=\"model\", barmode=\"group\", width=700, height=350)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you see a difference above in output prompt average sizing comparing RAG and FT?\n",
    "+ For input prompts it is quite obvious: Prompt size is bigger for RAG as the solution incorporates context text into the prompt.\n",
    "+ For output it is not so obvious, but you can see that fine-tuned mean prompt size is usual smaller. This is supported by research results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning scenario\n",
    "+ An application developer customizes the `Llama3 8B Chat` Pretrained (8B) model using 1000 tokens of data.\n",
    "+ After training, uses custom model provisioned throughput for 1 hour to evaluate the performance of the model. \n",
    "+ The fine-tuned model is stored for 1 month. \n",
    "+ After evaluation, the developer uses provisioned throughput (1mo commit) to host the customized model.\n",
    "\n",
    "#### Fixed fine-tuning training cost\n",
    "For each fine-tuning task we consider:\n",
    "+ Fine tuning training cost \n",
    "+ Fine-tuned model storage per month \n",
    "+ 1 hour of custom model inference for performance evaluation\n",
    "\n",
    "#### Monthly Provisioned Throughput pricing\n",
    "An application developer buys one model unit  for their text summarization use case.\n",
    "\n",
    "+ Total monthly cost incurred = 1 model unit * inference throughput cost (ex. $21.18 * 24 hours * 31 days )\n",
    "\n",
    "---\n",
    "\n",
    "**Note: We will be considering one year period of cummulative costs in this analysis but the time period can be asily changed to consider dsifferent timeframes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeframe considered\n",
    "start_date = '1/1/2023'\n",
    "end_date = '12/31/2023'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning training cost (fixed cost) considering it took 1 month\n",
    "num_hours_month = 744 # 24h * 31 days\n",
    "price_per_token = 0.00799\n",
    "#number_of_steps = 500\n",
    "#batch_size = 64\n",
    "dataset_size = 1000000 # number of tokens\n",
    "model_storage_per_month = 1.95 # 1 month\n",
    "evaluation_throuput_hour = 21.18 # 1 hour thrtoughput for model evaluation\n",
    "\n",
    "ft_training_cost = price_per_token * dataset_size + model_storage_per_month + evaluation_throuput_hour\n",
    "print(\"Fine-tuned fixed training cost: \", ft_training_cost)\n",
    "\n",
    "\n",
    "# fine-tuned model througput cost\n",
    "ft_model_hosting_cost_month = model_storage_per_month\n",
    "ft_token_cost_per_month = evaluation_throuput_hour * num_hours_month + model_storage_per_month\n",
    "\n",
    "print(\"Fine-tuned model monthly hosting cost: \", ft_model_hosting_cost_month)\n",
    "print(\"Fine-tuned model throuput cost: \", ft_token_cost_per_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG scenario\n",
    "We are considering Bedrock using a knoweledge base supported by Kendra for pricing simulation here.\n",
    "An application developer uses a pre-trained `Llama3 8B Chat` model supported by RAG with the following premisses: \n",
    "+ 90,000 documents in the knowledge base \n",
    "+ 7000 searches per day\n",
    "+ As we have less than 8k queries per day and 100k documents, the priccing per hour is $1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG hosting calculation (simulating Amazon Kendra)\n",
    "\n",
    "# define typical file size and that it is in Kendra (RAG)\n",
    "num_rag_doc_to_search = 90000 # just to see it is below 100K for pricing\n",
    "num_searches_day = 7000\n",
    "pricing_per_hour = 1.4 # Up to 8k queries per day, Up to 100k documents, \n",
    "# RAG monthly model host cost\n",
    "rag_hosting_princing_per_month = pricing_per_hour * num_hours_month # $1.4 per hour x 744 hours/month\n",
    "print(\"RAG monthly hosting cost: \", rag_hosting_princing_per_month)\n",
    "# RAG monthly model througput cost\n",
    "rag_token_cost_per_month = evaluation_throuput_hour * num_hours_month + model_storage_per_month\n",
    "print(\"RAG monthly throughput cost: \", rag_token_cost_per_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting RAG princing\n",
    "First, we look at at the different comntribution between RAG datasets hosting and tokens usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test stacke for FT and RAg separatedly\n",
    "# FT frequency - monthly\n",
    "def plot_pricing_rag_stacked(df):\n",
    "    fig = px.bar(df, title=\"RAG stacked pricing\",  x=df.month,  y=[\"RAG_hosting_monthly_pricing\",\"RAG_tokens_monthly_pricing\"])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rag_costs_stacked: month, RAG_hosting_monthly, RAG_tokens_monthly\n",
    "def prep_rag(plot=True):\n",
    "    num_months = pd.date_range(start=start_date, end=end_date, freq='ME').to_frame().shape[0]\n",
    "\n",
    "    l_accum_rag_token_pricing = []\n",
    "    l_accum_rag_hosting_pricing = []\n",
    "    rag_hosting_accum = 0\n",
    "    rag_token_accum = 0\n",
    "\n",
    "    for i in range(num_months):\n",
    "        rag_hosting_accum += rag_hosting_princing_per_month\n",
    "        rag_token_accum += rag_token_cost_per_month\n",
    "\n",
    "        l_accum_rag_token_pricing.append(rag_token_accum)\n",
    "        l_accum_rag_hosting_pricing.append(rag_hosting_accum)\n",
    "\n",
    "    # create dataframe for ploting\n",
    "    dt = pd.date_range(start=start_date, end=end_date, freq='ME')\n",
    "    dt.to_frame()\n",
    "    df_rag = dt.to_frame(index=False)\n",
    "    df_rag.rename(columns={0: \"month\"}, inplace=True)\n",
    "    df_rag['RAG_hosting_monthly_pricing'] = l_accum_rag_hosting_pricing\n",
    "    df_rag['RAG_tokens_monthly_pricing'] = l_accum_rag_token_pricing\n",
    "\n",
    "    if plot==True:\n",
    "        plot_pricing_rag_stacked(df_rag)\n",
    "    return df_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameter: frequency of fine-tuning training (months)\n",
    "df_rag = prep_rag(plot=True)\n",
    "df_rag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis considerations\n",
    "We can see that throughput contributes much more than hosting costs in the cummulative costs of a RAG solution.\n",
    "\n",
    "So the decision should be driven by efforts to reduce token costs.\n",
    "\n",
    "**For example, if you have a yearly budget of $\\$150K$, this use case DOESN'T fit on your budget.\n",
    "Probably using fine-tuning can be an alternative**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting fine-tuning princing\n",
    "Now, we look at at the different comntribution between fine-tuning fixed and monthly costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a first look at the different contributions between:\n",
    "+ fine-tuning fixed training cost\n",
    "+ fine-tuning monthly hosting cost\n",
    "+ fine-tuning monthly token cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test stacke for FT and RAg separatedly\n",
    "# FT frequency - monthly\n",
    "def plot_pricing_ft_stacked(every_n_months, df):\n",
    "    fig = px.bar(df, title=\"Fine-tuned stacked pricing - training frequency: every \" + str(every_n_months) + \" months\",\n",
    "                 x=df.month,  \n",
    "                 y=[\"FT_hosting_monthly_pricing\",\"FT_tokens_monthly_pricing\", \"FT_fixed_training_pricing\"])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing costs considering monthly fine-tuning with same dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_months = pd.date_range(start=start_date, end=end_date, freq='ME').to_frame().shape[0]\n",
    "\n",
    "def prep_finetuned(every_n_months, plot=True):\n",
    "    num_months = pd.date_range(start=start_date, end=end_date, freq='ME').to_frame().shape[0]\n",
    "    l_accum_ft_training_pricing = []\n",
    "    l_accum_ft_token_pricing = []\n",
    "    l_accum_ft_hosting_pricing = []\n",
    "    ft_hosting_accum = 0\n",
    "    ft_token_accum = 0\n",
    "    ft_training_accum = 0\n",
    "    # here we can change the training frequency and see how it affects the pricing\n",
    "    #every_n_months = 4\n",
    "    #every_n_months += m\n",
    "    for i in range(num_months):\n",
    "        #Accumulate fixed fine-tuning according to periodicity\n",
    "        if i % every_n_months == 0:\n",
    "            ft_training_accum += ft_training_cost\n",
    "        \n",
    "        ft_hosting_accum += ft_model_hosting_cost_month\n",
    "        ft_token_accum += ft_token_cost_per_month\n",
    "        l_accum_ft_token_pricing.append(ft_token_accum) # CASS REMOVE LATER: Multiplying tokens for n to show in plot\n",
    "        l_accum_ft_hosting_pricing.append(ft_hosting_accum)\n",
    "        l_accum_ft_training_pricing.append(ft_training_accum)\n",
    "    # create dataframe for ploting\n",
    "    dt = pd.date_range(start=start_date, end=end_date, freq='ME')\n",
    "    dt.to_frame()\n",
    "    df_ft = dt.to_frame(index=False)\n",
    "    df_ft.rename(columns={0: \"month\"}, inplace=True)\n",
    "    df_ft['FT_hosting_monthly_pricing'] = l_accum_ft_hosting_pricing\n",
    "    df_ft['FT_tokens_monthly_pricing'] = l_accum_ft_token_pricing\n",
    "    df_ft['FT_fixed_training_pricing'] = l_accum_ft_training_pricing\n",
    "    if plot==True:\n",
    "        plot_pricing_ft_stacked(every_n_months,df_ft)\n",
    "    return df_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input parameter: frequency of fine-tuning training (months)\n",
    "df_ft_1 = prep_finetuned(1)\n",
    "df_ft_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis considerations\n",
    "We can see that training costs are insignificant when compared to throughput ones. \n",
    "\n",
    "And also that, for the same use case used here for RAG, the cummulative costs have a similar range.\n",
    "\n",
    "So far, there is not difference when choosing between RAG or fine-tude solution.\n",
    "\n",
    "But let's explore different fine-tune training frequencies and see how it affects cumulative costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring different fine-tuning frequencies\n",
    "Let's first consider fine-tuning every quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input parameter: frequency of fine-tuning training (months)\n",
    "df_ft_3 = prep_finetuned(3)\n",
    "df_ft_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now considering fine-tuning semi-yearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input parameter: frequency of fine-tuning training (months)\n",
    "df_ft_6 = prep_finetuned(6)\n",
    "df_ft_6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input parameter: frequency of fine-tuning training (months)\n",
    "df_ft_12 = prep_finetuned(12)\n",
    "df_ft_12.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis considerations\n",
    "We can clearly see that the fine-tuning frequency has a significant impact on total costs depending on the frequency. \n",
    "Considering the final yearly costs, we can see a reduction of around $\\$90k$ (from $\\$96K$ to $\\$8K) when reducing finetune-frequency from once a month to annualy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting fine-tuning frequency against cummulative annual training costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft_1['frequency'] = \"monthly\"\n",
    "df_ft_3['frequency'] = \"quarterly\"\n",
    "df_ft_6['frequency'] = \"semi-annualy\"\n",
    "df_ft_12['frequency'] = \"annualy\"\n",
    "df_rag['frequency'] = \"RAG\"\n",
    "\n",
    "all_df = [df_ft_1, df_ft_3, df_ft_6, df_ft_12]\n",
    "final_df = pd.concat(all_df, ignore_index=True)\n",
    "\n",
    "fig = px.line(final_df, x=\"month\", y=\"FT_fixed_training_pricing\", color='frequency')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting total cummulative costs for RAG and different training frequency for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum RAG costs\n",
    "df_rag[\"cummulative_cost\"] = df_rag[\"RAG_hosting_monthly_pricing\"] + df_rag[\"RAG_tokens_monthly_pricing\"]\n",
    "# sum FT costs\n",
    "df_ft_1[\"cummulative_cost\"] = df_ft_1[\"FT_hosting_monthly_pricing\"] + df_ft_1[\"FT_tokens_monthly_pricing\"] + \\\n",
    "                                df_ft_1[\"FT_fixed_training_pricing\"]\n",
    "df_ft_3[\"cummulative_cost\"] = df_ft_3[\"FT_hosting_monthly_pricing\"] + df_ft_3[\"FT_tokens_monthly_pricing\"] +  \\\n",
    "                                df_ft_3[\"FT_fixed_training_pricing\"]\n",
    "df_ft_6[\"cummulative_cost\"] = df_ft_6[\"FT_hosting_monthly_pricing\"] + df_ft_6[\"FT_tokens_monthly_pricing\"] + \\\n",
    "                                df_ft_6[\"FT_fixed_training_pricing\"]\n",
    "df_ft_12[\"cummulative_cost\"] = df_ft_12[\"FT_hosting_monthly_pricing\"] + df_ft_12[\"FT_tokens_monthly_pricing\"] + \\\n",
    "                                df_ft_12[\"FT_fixed_training_pricing\"]\n",
    "\n",
    "all_df = [df_ft_1, df_ft_3, df_ft_6, df_ft_12, df_rag]\n",
    "final_df = pd.concat(all_df, ignore_index=True)\n",
    "\n",
    "fig = px.line(final_df, x=\"month\", y=\"cummulative_cost\", color='frequency')\n",
    "fig.show()\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakeven analysis\n",
    "We can also perform a break-even analysis, i.e., when is the point where the cummulative costs of RAG achieve the same costs for the fine-tuned solution.\n",
    "\n",
    "For the plot above we can see that, for the period of 1 year considered, let's compare the yearly fine-tuned mopdel against the RAG one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft_vs_rag = final_df[(final_df.frequency == \"annualy\") | (final_df.frequency == \"RAG\")]\n",
    "# selecting only needed columns\n",
    "df_ft_vs_rag = df_ft_vs_rag[[\"month\", \"cummulative_cost\", \"frequency\"]]\n",
    "\n",
    "fig = px.line(df_ft_vs_rag, x=\"month\", y=\"cummulative_cost\", color='frequency')\n",
    "fig.show()\n",
    "df_ft_vs_rag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rag = df_ft_vs_rag.loc[(df_ft_vs_rag['frequency'] == \"RAG\")]\n",
    "df_monthly_ft = df_ft_vs_rag.loc[(df_ft_vs_rag['frequency'] == \"annualy\")]\n",
    "\n",
    "# merging both to calculate cost difference\n",
    "df_temp = pd.merge(df_rag, df_monthly_ft, on=['month', 'month'])\n",
    "\n",
    "df_temp = df_temp[['month', 'cummulative_cost_x', 'cummulative_cost_y']].copy()\n",
    "df_temp.head()\n",
    "df_temp[\"cost_diff\"] = df_temp[\"cummulative_cost_x\"] - df_temp[\"cummulative_cost_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding difference closest to zero \n",
    "import numpy as np\n",
    "df_closest = df_temp.iloc[(df_temp['cost_diff']-101).abs().argsort()[:1]]\n",
    "df_closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above shows that we achieve breakeven in costs around August 8th, 2023 for this use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate vertically this columns to dataset\n",
    "df_closest.drop([\"cummulative_cost_x\", \"cummulative_cost_y\"], axis=1, inplace=True)\n",
    "df_closest.columns = [\"month\", \"cummulative_cost\"]\n",
    "df_closest[\"frequency\"] = \"Breakeven\"\n",
    "df_closest\n",
    "\n",
    "# prepare to plot breakeven threshold\n",
    "df_closest = pd.DataFrame(np.repeat(df_closest.values, df_rag.shape[0], axis=0))\n",
    "df_closest.columns = [\"month\", \"cummulative_cost\", \"frequency\"]\n",
    "#seq = list(range(df_rag.shape[0])) \n",
    "seq = list(range(1,200000,18000)) # define a range to show up in the plot\n",
    "df_closest[\"cummulative_cost\"] = seq\n",
    "#df_closest\n",
    "\n",
    "both_df = [df_ft_vs_rag, df_closest]\n",
    "breakeven_df = pd.concat(both_df, ignore_index=True)\n",
    "#breakeven_df\n",
    "\n",
    "fig = px.line(breakeven_df, x=\"month\", y=\"cummulative_cost\", color='frequency')\n",
    "fig.show()\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis considerations\n",
    "We can see in this example that by the end of August the RAG accumulated costs achieve the cummulative costs for Fine-tuning considering monthly fine-tuning training. It means that you can choose to spend less up front not paying for finetune and use RAG instead, if you plan to fine-tune your modle monthly.\n",
    "\n",
    "If you opt for lower fine-tuning frequency, RAG is always a cheaper solution for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
