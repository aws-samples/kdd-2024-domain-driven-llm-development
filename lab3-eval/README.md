# Lab 3: RAG + Fine-tuning and Benchmarking     
This lab evaluates the performance of RAG, fine-tuned LLM, and a combined architecture on domain-specific question-answer and summarization tasks. We test these approaches using a consistent datasets and performance metrics. Initial metric outcomes guide the decision on whether either approach meets the expected performance. If so, we further estimate the costs associated with each approach, and conduct ROI analysis to determine the most cost-performant solution.

To get started with this lab activity:
+ Navigate to the lab2-eval folder
+ Open the kdd2024-model-comparison.ipynb notebook

### Summary:   
This lab explore tools to support deciding between RAG and domain fine-tuning for LLM tasks such as Q&A or summarizing documents.

Based on results from Lab 1 - Advanced Techniques in Retrieval Augmented Generation (RAG), and Lab 2 - LLM fine-tuning, you wil explore how you can compare both approaches to support decision making related to which solution to choose.

Decision making related to choosing between Fine-tuned and RAG models performances should take into account model performance as well as pricing.

In the first session of this notebook you compare model performance to assess if one or both models achieve a minimum acceptable performance based on appropriate metrics related to the task on hand.

The second session is about offering pricing analysis tools to help decide between both models.
